{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orders Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tkr\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', 9999)\n",
    "pd.set_option('display.max_rows', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the csv file and assign it to a dataframe.\n",
    "df = pd.read_csv('order_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate the data type of each column\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values in each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take an initial look at the set up of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date']).dt.date #Remove the time portion of the date\n",
    "df['date'] = pd.to_datetime(df['date']) #Convert the date column from a string to a date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that the date column type changed to a date\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine the top of the dataframe\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine the bottom of the dataframe\n",
    "df.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Assemble a dataframe with one row per customer and the following columns:\n",
    "    * customer_id\n",
    "    * gender\n",
    "    * most_recent_order_date\n",
    "    * order_count (number of orders placed by this customer)\n",
    "   Sort the dataframe by customer_id ascending and display the first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine two customer id's before grouping to use as a check\n",
    "df[(df['customer_id']==8658) | (df['customer_id']==5989)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataframe for grouping\n",
    "df_new = df.groupby(['customer_id','gender'], sort=True).agg({'date': 'last','value': 'count'})\n",
    "df_new = df_new.reset_index() #Set an index for the dataframe\n",
    "df_new.rename(columns={'date':'most_recent_order','value': 'total_orders',}, inplace=True) #Renamed two of the columns\n",
    "df_new.head(10) #Display the first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examined the same customer id's to make sure the grouping and calculation worked correctly\n",
    "df_new[(df_new['customer_id']==8658) | (df_new['customer_id']==5989)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Plot the count of orders per week for the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a separate dataframe of dates and sales\n",
    "sales = df[['date','value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the date to be the index of the new dataframe and sort it by the date\n",
    "sales.set_index('date', inplace=True)\n",
    "sales.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Examine the new dataframe\n",
    "sales.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the dataframe to show sales by week\n",
    "sales = sales.resample('W').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine the dataframe after setting the index and grouping sales by week\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used a function to show the value columns with commas\n",
    "def func(x, pos):  \n",
    "   s = '{:0,d}'.format(int(x))\n",
    "   return s\n",
    "\n",
    "y_format = tkr.FuncFormatter(func)\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.grid(b=None)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel('Sales($)', fontsize=25)\n",
    "plt.title('Weekly Order Count',fontsize=20)\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(sales['value'],linewidth=5,color='red');\n",
    "ax.yaxis.set_major_formatter(y_format)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Compute the mean order value for gender 0 and for gender 1. Do you think the difference is significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouped by gender and calculated the mean of the value column, rounded to two places\n",
    "pd.DataFrame(round(df.groupby('gender')['value'].mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculated the t-statistic and p-value to check for signifigance\n",
    "stats.ttest_ind(df[df['gender']==0]['value'],df[df['gender']==1]['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is 0.048.  Using an alpha (significance level) of 0.05, I do not think that there is  a significant difference in the mean order values for the two genders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D) Assuming a single gender prediction was made for each customer, generate a confusion matrix for predicted gender. What does the confusion matrix tell you about the quality of the predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(df['gender'],df['predicted_gender'])\n",
    "tn, fp, fn, tp = confusion_matrix(df['gender'],df['predicted_gender']).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('True 0: ', tn)\n",
    "print('False 1: ', fp)\n",
    "print('False 0: ', fn)\n",
    "print('True 1: ', tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification metrics\n",
    "accuracy = round((((tp+tn)/(tp+fn+tn+fp))*100),2)\n",
    "misclassification = round(((1 - (accuracy/100))*100),2)\n",
    "sensitivy = round(((tp/(tp+fn))*100),2)\n",
    "specificity = round(((tn/(tn+fp))*100),2)\n",
    "precision = round(((tp/(tp+fp))*100),2)\n",
    "print('Accuracy Rate:',accuracy,'%')\n",
    "print('Misclassification Rate:',misclassification,'%')\n",
    "print('Sensitivy Rate:',sensitivy,'%')\n",
    "print('Specificity Rate:',specificity,'%')\n",
    "print('Precision Rate:',precision,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(cm, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check value counts for each of the genders\n",
    "pd.DataFrame(df['gender'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score is about 64%.  Depending on your tolerance this may be a good score.  If possible, I might prefer to see a score closer to 80%.  Looking at the confusion matrix table, it appears that the model does a better job of predicting gender class 1 than it does predicting class 0.  The model correctly predicts class 0 about 50% of the time while it correctly predicts class 1 about 78% of the time.  I would want to further investigate why the model is better at predicting one class over the other especially because the classes are roughly balanced (there is very little bias to one class over the other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
